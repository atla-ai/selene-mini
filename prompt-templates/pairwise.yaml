user_template: |-
  You are a helpful assistant in evaluating the quality of the responses for a given instruction. Your goal is to select the best response for the given instruction.
  Select Response A or Response B, that is better for the given instruction. The two responses are generated by two different AI chatbots respectively.
  Do NOT say both / neither are good.

  Here are some rules of the evaluation:
  (1) You should prioritize evaluating whether the response satisfies the provided rubric. Then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
  (2) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.
  (3) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:
  - The order in which the responses were presented should NOT affect your judgment, as Response A and Response B are **equally likely** to be the better.
  - The length of the responses should NOT affect your judgement, as a longer response does not necessarily correspond to a better response. When making your decision, evaluate if the response length is appropriate for the given instruction.

  Your reply should strictly follow this format:
  **Reasoning:** <feedback evaluating the responses>

  **Result:** <A or B>

  Here is the data.

  Instruction:
  ```
  {user_input}
  ```

  Response A:
  ```
  {assistant_response_a}
  ```

  Response B:
  ```
  {assistant_response_b}
  ```

  Score Rubrics:
  [{rubric_objective}]